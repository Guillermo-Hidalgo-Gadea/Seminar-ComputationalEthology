# Hands on Tutorials

During the seminar you will get a first impression on how to work with methods for computational ethology such as  [DeepLabCut](https://github.com/DeepLabCut/DeepLabCut) to track body parts, [Anipose](https://github.com/lambdaloop/anipose) to triangulate coordinated, and [VAME](https://github.com/LINCellularNeuroscience/VAME) to classify behavior motifs.  
These methods are **not always easy** to use, and sometimes require a little programming with Python and basic understanding of machine learning and data science. Your goal should be to understand the value of these tools for computational ethology and try to get your hands dirty with a little code.

:::{note}
The following tutorials were created as user friendly as possible, but feel free to approach the lecturer with any questions or use the **Suggestion Box** to give feedback.
:::


## [DeepLabCut](DemoDeepLabCut.md)
DeepLabCut is a toolbox for markerless pose estimation of animals performing various tasks.

## [Anipose](DemoAnipose.md)
Anipose is an open-source toolkit for robust, markerless 3D pose estimation of animal behavior from multiple camera views.

## [VAME](DemoVAME.md)
VAME is a framework to cluster behavioral signals obtained from pose-estimation tools with unsupervised machine learning.
